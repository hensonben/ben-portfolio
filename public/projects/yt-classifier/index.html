<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>YouTube Comment Classifier | Benjamin Henson</title>
<meta name="keywords" content="">
<meta name="description" content="Overview
This project fine-tunes a DistilBERT Transformermodel to classify YouTube comments into positive, neutral, or negative sentiments.
The goal was to gain hands-on experience with Natural Language Processing (NLP) and explore the process of fine-tuning pretrained language models using Hugging Face Transformers and PyTorch.
The dataset was sourced from Kaggle and consists of real YouTube user comments labeled by sentiment. See end of page for a link to the dataset if so inclined.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/projects/yt-classifier/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/projects/yt-classifier/index.xml" title="rss">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/yt-classifier/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }
    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/custom.css" />
<header class="navbar">
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/about/">About</a></li>
    <li><a href="/projects/">Projects</a></li>
    <li><a href="/contact/">Contact</a></li>
  </ul>
</header>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-mLLfwC3roQoPCaC9HIcA3zD8uR+ymcU+nMMT+X2SVI33NnEw5e99bGyADf8p+8db" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-z3bOTo6a5x1EdtSxRwLKtB3bWucCJQ2eN7qvJkMTvhvE8DaRUT8yqRpxW6fUR0Hk" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-mllN6pFyz9ZxdpKxZBvTTQ9bRkOa5GqJycGP4sdFZbQvRkLMRnKx2y8c5jDqJemr" crossorigin="anonymous"
    onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '\\[', right: '\\]', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });">
</script>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    tags: 'ams'
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  svg: { fontCache: 'global' }
};
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body class="list dark" id="top">


    
    <main class="main"> 
<header class="page-header">
  <h1>
    YouTube Comment Classifier
  </h1>
</header>
<div class="post-content"><h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>This project fine-tunes a <strong>DistilBERT Transformer</strong>model to classify YouTube comments into positive, neutral, or negative sentiments.
The goal was to gain hands-on experience with <strong>Natural Language Processing (NLP)</strong> and explore the process of fine-tuning pretrained language models using <strong>Hugging Face Transformers</strong> and <strong>PyTorch</strong>.</p>
<p>The dataset was sourced from Kaggle and consists of real YouTube user comments labeled by sentiment. <em>See end of page for a link to the dataset if so inclined.</em></p>
<h3 id="tools-and-libraries">Tools and Libraries<a hidden class="anchor" aria-hidden="true" href="#tools-and-libraries">#</a></h3>
<ul>
<li>Python 3.13</li>
<li>Hugging Face Transformers (v4.57.1)</li>
<li>Datasets (Hugging Face)</li>
<li>PyTorch 2.8.0</li>
<li>Accelerate 1.10.1</li>
</ul>
<h3 id="model-and-training-details">Model and Training Details<a hidden class="anchor" aria-hidden="true" href="#model-and-training-details">#</a></h3>
<p>After preprocessing, the text was <strong>tokenized</strong> and converted into numerical input for the model. Additionally, sentiment labels were encoded as integers (0 = negative, 1 = neutral, 2 = positive)</p>
<h3 id="model-distilbert-base-uncased">Model: <code>distilbert-base-uncased</code><a hidden class="anchor" aria-hidden="true" href="#model-distilbert-base-uncased">#</a></h3>
<p>This is a lightweight version of <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) that retains most of BERT&rsquo;s accuracy while being both faster and smaller. Additionally, it&rsquo;s pretrained on large text data and fine-tuned here for sentiment classification. This project allowed the model to learn, rather than strictly memorize, the nuance and emotion (think sarcasm, humor, etc.) behind words in a given sentence.<br>
<a href="/projects/yt-classifier/_bert-info/">More on BERT</a></p>
<h3 id="task-sequence-classification">Task: <em>Sequence Classification</em><a hidden class="anchor" aria-hidden="true" href="#task-sequence-classification">#</a></h3>
<p>The model learns to assign one of three sentiment categories (positive, neutral, or negative) to each input text sequence (YouTube comment).</p>
<h3 id="traintest-split-8020">Train/Test Split: <em>80/20</em><a hidden class="anchor" aria-hidden="true" href="#traintest-split-8020">#</a></h3>
<p>80% of the dataset is used for training the model, while 20% is reserved for evaluating how well it generalizes to unseen data.</p>
<h3 id="batch-size-8">Batch Size: <em>8</em><a hidden class="anchor" aria-hidden="true" href="#batch-size-8">#</a></h3>
<p>During training, the model processes 8 examples at a time before updating its weights. Smaller batch sizes can help with limited GPU memory and improve generalization slightly.</p>
<h3 id="learning-rate-2e-5">Learning Rate: <code>2e-5</code><a hidden class="anchor" aria-hidden="true" href="#learning-rate-2e-5">#</a></h3>
<p>Controls how much the model’s weights are adjusted during training.<br>
A smaller learning rate helps prevent the model from “overshooting” the optimal solution.</p>
<h3 id="epochs-2">Epochs: <em>2</em><a hidden class="anchor" aria-hidden="true" href="#epochs-2">#</a></h3>
<p>An <em>epoch</em> represents one complete pass through the entire training dataset.<br>
After two passes, the model’s performance had plateaued, indicating that additional training would not meaningfully improve results.</p>
<h3 id="weight-decay-001">Weight Decay: <em>0.01</em><a hidden class="anchor" aria-hidden="true" href="#weight-decay-001">#</a></h3>
<p>A regularization technique that prevents overfitting by slightly penalizing large weights during optimization. This encourages the model to maintain simpler, more generalizable parameter values.</p>
<h3 id="optimizer-adamw">Optimizer: <em>AdamW</em><a hidden class="anchor" aria-hidden="true" href="#optimizer-adamw">#</a></h3>
<p>A variant of the Adam optimizer that integrates weight decay directly into the update rule.<br>
It is widely used for fine-tuning Transformer-based models such as BERT and DistilBERT, providing stable convergence and reduced overfitting.<br>
<a href="/projects/yt-classifier/_adam-info/">More on Adam</a></p>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th style="text-align: center">Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Training Loss</strong></td>
          <td style="text-align: center">0.37</td>
      </tr>
      <tr>
          <td><strong>Evaluation Loss</strong></td>
          <td style="text-align: center">0.49</td>
      </tr>
      <tr>
          <td><strong>Accuracy</strong></td>
          <td style="text-align: center">0.84</td>
      </tr>
      <tr>
          <td><strong>Weighted F1 Score</strong></td>
          <td style="text-align: center">0.84</td>
      </tr>
  </tbody>
</table>
<p>Overall the model achieved strong performance on unseen YouTube comments, which demonstrated effective transfer learning from pretrained weights to real-world text.</p>
<h3 id="model-access">Model Access<a hidden class="anchor" aria-hidden="true" href="#model-access">#</a></h3>
<p>You can test and download the fine-tuned model directly on Hugging Face:<br>
<a href="https://huggingface.co/azande7/yt-sentiment-model">https://huggingface.co/azande7/yt-sentiment-model</a></p>
<p>Link to the dataset: (<a href="https://www.kaggle.com/datasets/atifaliak/youtube-comments-dataset">https://www.kaggle.com/datasets/atifaliak/youtube-comments-dataset</a>)</p>


</div>
    </main>

    
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Benjamin Henson</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters:[{left:'$$', right:'$$', display:true}, {left:'$', right:'$', display:false}, {left:'\\\\(', right:'\\\\)', display:false}, {left:'\\\\[', right:'\\\\]', display:true}]});"></script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>
</html>