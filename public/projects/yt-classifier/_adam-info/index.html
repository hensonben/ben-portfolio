<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>More on Adam | Benjamin Henson</title>
<meta name="keywords" content="">
<meta name="description" content="Adam and AdamW Optimizers
Optimization algorithms are essential for training deep learning models. They determine how model parameters (weights and biases) are updated during backpropagation to minimize the loss function.
Among the most widely used optimizers in modern deep learning are Adam and its improved variant AdamW. Both are based on adaptive learning rate methods that adjust how much each parameter changes based on historical gradients.

The Adam Optimizer
Adam (short for Adaptive Moment Estimation) was introduced by Diederik P. Kingma and Jimmy Ba in 2014.
It combines the strengths of two earlier methods; AdaGrad (adaptive learning rates for sparse data) and RMSProp (exponential moving average of squared gradients).
See More">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/projects/yt-classifier/_adam-info/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/yt-classifier/_adam-info/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }
    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/custom.css" />
<header class="navbar">
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/about/">About</a></li>
    <li><a href="/projects/">Projects</a></li>
    <li><a href="/contact/">Contact</a></li>
  </ul>
</header>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-mLLfwC3roQoPCaC9HIcA3zD8uR+ymcU+nMMT+X2SVI33NnEw5e99bGyADf8p+8db" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-z3bOTo6a5x1EdtSxRwLKtB3bWucCJQ2eN7qvJkMTvhvE8DaRUT8yqRpxW6fUR0Hk" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-mllN6pFyz9ZxdpKxZBvTTQ9bRkOa5GqJycGP4sdFZbQvRkLMRnKx2y8c5jDqJemr" crossorigin="anonymous"
    onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '\\[', right: '\\]', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });">
</script>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    tags: 'ams'
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  svg: { fontCache: 'global' }
};
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
</head>

<body class=" dark" id="top">


    
    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      More on Adam
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="adam-and-adamw-optimizers">Adam and AdamW Optimizers<a hidden class="anchor" aria-hidden="true" href="#adam-and-adamw-optimizers">#</a></h1>
<p>Optimization algorithms are essential for training deep learning models. They determine how model parameters (weights and biases) are updated during backpropagation to minimize the loss function.</p>
<p>Among the most widely used optimizers in modern deep learning are <strong>Adam</strong> and its improved variant <strong>AdamW</strong>. Both are based on adaptive learning rate methods that adjust how much each parameter changes based on historical gradients.</p>
<hr>
<h2 id="the-adam-optimizer">The Adam Optimizer<a hidden class="anchor" aria-hidden="true" href="#the-adam-optimizer">#</a></h2>
<p><strong>Adam</strong> (short for <em>Adaptive Moment Estimation</em>) was introduced by Diederik P. Kingma and Jimmy Ba in 2014.<br>
It combines the strengths of two earlier methods; <strong>AdaGrad</strong> (adaptive learning rates for sparse data) and <strong>RMSProp</strong> (exponential moving average of squared gradients).<br>
<a href="https://arxiv.org/abs/1412.6980">See More</a></p>
<h3 id="core-concepts">Core Concepts<a hidden class="anchor" aria-hidden="true" href="#core-concepts">#</a></h3>
<p>Adam maintains two moving averages for each parameter during training:</p>
<ol>
<li>
<p><strong>First moment (mean of gradients):</strong><br>
Tracks the average direction of updates.</p>
</li>
<li>
<p><strong>Second moment (uncentered variance):</strong><br>
Tracks the magnitude of gradients to scale learning adaptively.</p>
</li>
</ol>
<p>These are computed as:</p>
<p>$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$</p>
<p>$$<br>
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$</p>
<ul>
<li>
<p>g sub t is the gradient at time step (t)</p>
</li>
<li>
<p>beta 1 and beta 2 are decay rates (typically 0.9 and 0.999)</p>
</li>
<li>
<p>m sub t and v sub t are bias-corrected before being applied.</p>
</li>
</ul>
<p>The parameter update rule is then:</p>
<p>$$
\theta_t = \theta_{t-1} - \alpha \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
$$</p>
<p>where:</p>
<ul>
<li>alpha is the learning rate</li>
<li>epsilon is a small constant for numerical stability</li>
</ul>
<h3 id="advantages">Advantages<a hidden class="anchor" aria-hidden="true" href="#advantages">#</a></h3>
<ul>
<li><strong>Adaptive learning rates:</strong> Each parameter adjusts based on its own gradient history.</li>
<li><strong>Fast convergence:</strong> Works well for large-scale, noisy datasets.</li>
<li><strong>Minimal tuning:</strong> Performs reliably with default parameters.</li>
</ul>
<h3 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h3>
<p>Despite its effectiveness, the original Adam algorithm can sometimes <strong>generalize poorly</strong> compared to SGD.<br>
This happens because weight decay (regularization) is coupled with adaptive learning rates, leading to inconsistent regularization strength, a problem that <strong>AdamW</strong> addresses.</p>
<hr>
<h2 id="the-adamw-optimizer">The AdamW Optimizer<a hidden class="anchor" aria-hidden="true" href="#the-adamw-optimizer">#</a></h2>
<p><strong>AdamW</strong> (short for <em>Adam with Decoupled Weight Decay</em>) was introduced by Ilya Loshchilov and Frank Hutter in 2017.<br>
It modifies Adam by <strong>decoupling weight decay from the gradient-based update rule</strong>, improving both training stability and generalization.<br>
<a href="https://arxiv.org/abs/1711.05101">See More</a></p>
<h3 id="core-idea">Core Idea<a hidden class="anchor" aria-hidden="true" href="#core-idea">#</a></h3>
<p>In standard Adam, weight decay was implemented by adding the L2 regularization term directly into the gradient computation. This unintentionally made weight decay <strong>dependent on the adaptive learning rate</strong>, distorting the intended regularization effect.</p>
<p>AdamW fixes this by applying weight decay as a <strong>separate step</strong> after gradient updates:</p>
<p>$$
\theta_t = \theta_{t-1} - \alpha \left( \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon} + \lambda \theta_{t-1} \right)
$$</p>
<p>where:</p>
<ul>
<li>lambda is the weight decay coefficient</li>
<li>alpha is the learning rate</li>
</ul>
<p>This separation ensures consistent regularization strength regardless of learning rate adjustments.</p>
<h3 id="benefits-of-adamw">Benefits of AdamW<a hidden class="anchor" aria-hidden="true" href="#benefits-of-adamw">#</a></h3>
<ul>
<li><strong>Improved generalization:</strong> Matches or exceeds SGD performance on large-scale tasks (e.g., ImageNet, NLP fine-tuning).</li>
<li><strong>Stable training:</strong> Decoupling weight decay prevents unintended interference with adaptive gradients.</li>
<li><strong>Standardization:</strong> Adopted as the default optimizer for Transformer-based architectures (e.g., BERT, RoBERTa, GPT).</li>
</ul>
<p><a href="/projects/yt-classifier/">← Back to previous page</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>

    
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Benjamin Henson</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body, {delimiters:[{left:'$$', right:'$$', display:true}, {left:'$', right:'$', display:false}, {left:'\\\\(', right:'\\\\)', display:false}, {left:'\\\\[', right:'\\\\]', display:true}]});"></script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>
</html>